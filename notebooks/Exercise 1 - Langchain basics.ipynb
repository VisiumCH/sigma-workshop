{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agentic fighters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1 - LangChain basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 - Chatbot simple\n",
    "\n",
    "Bienvenidos a la primera parte del workshop!\n",
    "\n",
    "Vamos a empezar viendo cómo podemos generar una aplicación basada en IA que utilice LangChain, un framework muy popular para desarrollar apps con LLMs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparar las claves\n",
    "\n",
    "Primero, vamos a preparar el entorno de Python para poder usar las claves de OpenAI. Hay que definir la OPENAI_API_KEY en el archivo .env.\n",
    "\n",
    "El código busca la clave y fija unos parámetros:\n",
    "\n",
    "- LLM_MODEL: el modelo a utilizar\n",
    "- LLM_TEMPERATURE: parámetro que controla la aleatoriedad de las respuestas (0 significa que será completamente determinista)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
    "    raise ValueError(\"Please set OPENAI_API_KEY environment variable\")\n",
    "\n",
    "print(os.environ.get(\"OPENAI_API_KEY\"))\n",
    "\n",
    "LLM_MODEL = \"gpt-4o-mini\"\n",
    "LLM_TEMPERATURE = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora vamos a crear el chabot con LangChain. Usaremos:\n",
    "\n",
    "- ChatOpenAI: la interfaz a los modelos de OpenAI\n",
    "- SystemMessage: define el comportamiento general del modelo\n",
    "- HumanMessage: representa el input del usuario\n",
    "\n",
    "Vamos a crear un chatbot con la temática deseada. Para ello:\n",
    "\n",
    "1. instanciamos el modelo\n",
    "2. definimos el system prompt que define el rol del chatbot\n",
    "3. enviamos una query y recibimos una respuesta con el método .invoke()\n",
    "\n",
    "Así vemos el patrón básico de interacciones: prompt → respuesta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create a ChatOpenAI instance with the LLM model and temperature\n",
    "base_model = ChatOpenAI(model=LLM_MODEL, temperature=LLM_TEMPERATURE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PROMPT = \"\"\"\n",
    "(Set system prompt)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Request from the client\n",
    "request = \" (Request to be done) \"\n",
    "\n",
    "# Message list for the base model\n",
    "messages = [\n",
    "    SystemMessage(BASE_PROMPT),\n",
    "    HumanMessage(request),\n",
    "]\n",
    "print(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invoke the model with the messages\n",
    "response = base_model.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Markdown(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 - Prompt template"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LangChain proporciona formas de manejar los prompts, para ser consistente y con ello poder parsear resultados convenientemente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template_string = \"\"\"Traduce el texto entre comillas simples al {target_language}:\n",
    "'{input_text}'\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "prompt_template = ChatPromptTemplate.from_template(template_string)\n",
    "prompt_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template.messages[0].prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template.messages[0].prompt.input_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_language = \"English\"\n",
    "input_text = \"Estamos haciendo un workshop en la UPV para aprender sobre agentes y poder desarrollar una app generativa de peleas!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "message = prompt_template.format_messages(input_text=input_text, target_language=target_language)\n",
    "print(type(message))\n",
    "message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "message[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = base_model.invoke(message)\n",
    "print(response)\n",
    "Markdown(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 - Parsers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esto nos devuelve una respuesta del tipo _AIMessage_. Pero vamos a ver si lo que queremos es parsearla y que devuelva un JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "prompt_template_json = copy.deepcopy(prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template_json.messages[0].prompt.template = f\"{prompt_template.messages[0].prompt.template}. Crea la respuesta en formato JSON, con claves 'idioma_original' y 'traducido'\"\n",
    "prompt_template_json.messages[0].prompt.template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "message = prompt_template_json.format_messages(input_text=input_text, target_language=target_language)\n",
    "response = base_model.invoke(message)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response.content.get(\"traducido\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No se ha generado un JSON, sino que es un string con un formato similar.\n",
    "\n",
    "Vamos a ver qué podemos hacer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import ResponseSchema, StructuredOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_schema = [\n",
    "    ResponseSchema(name=\"idioma_original\", description=\"The original text\"),\n",
    "    ResponseSchema(name=\"traducido\", description=\"The translated text\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_parser = StructuredOutputParser.from_response_schemas(response_schema)\n",
    "output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "format_intructions = output_parser.get_format_instructions()\n",
    "Markdown(format_intructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template_json_format = \"\"\"Traduce el texto entre comillas simples al {target_language}:\n",
    "'{input_text}'.\n",
    "\n",
    "{format_intructions}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(prompt_template_json_format)\n",
    "message = prompt.format_messages(input_text=input_text, target_language=target_language, format_intructions=format_intructions)\n",
    "print(type(message))\n",
    "message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = base_model.invoke(message)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dict = output_parser.parse(response.content)\n",
    "output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dict.get(\"traducido\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra: Chains -> LangChain Expression Language (LCEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las cadenas eran el elemento básico de LangChain, que nos permiten ejecutar prompts uno detrás de otro (o con la estructura definida).\n",
    "\n",
    "Hoy en día estan deprecadas. Son interesantes de conocer, pero ahora todo se basa en Runnables, que es lo que utiliza LangGraph. Pasamos directamente a ello."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
